#!/usr/bin/env python3
"""
Integrated Performance Optimization Validation Script

This script integrates all optimized autonomous components and validates 
that the <500ms performance target is achieved.
"""

import sys
import os
import time
import asyncio
from typing import Dict, List, Any
from datetime import datetime

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))

try:
    # Import optimized components
    from mcp_agent.autonomous.task_analyzer_optimized import TaskLevelCachedAnalyzer
    from mcp_agent.autonomous.tool_discovery_optimized import ParallelToolDiscoveryAgent
    from mcp_agent.autonomous.decision_engine_optimized import CachedAutonomousDecisionEngine
    from mcp_agent.autonomous.meta_coordinator_optimized import ResourceOptimizedMetaCoordinator
    
    # Import base components for comparison
    from mcp_agent.autonomous.task_analyzer import TaskAnalyzer as OriginalTaskAnalyzer
    from mcp_agent.autonomous.tool_discovery import ToolDiscoveryAgent as OriginalToolDiscovery
    from mcp_agent.autonomous.decision_engine import AutonomousDecisionEngine as OriginalDecisionEngine
    from mcp_agent.autonomous.meta_coordinator import MetaCoordinator as OriginalMetaCoordinator
    
    print("‚úÖ All optimized component imports successful")
    
except ImportError as e:
    print(f"‚ùå Import failed: {e}")
    sys.exit(1)


class OptimizationValidator:
    """Validates that all optimizations work together and meet performance targets."""
    
    def __init__(self):
        self.test_tasks = [
            "Create a simple Python script to read a CSV file",
            "Search for information about machine learning trends", 
            "Build a web scraper for product data",
            "Analyze sales data and create a report",
            "Set up a GitHub repository with documentation",
            "Query a database for customer information",
            "Automate email sending with attachments",
            "Research AI developments and summarize findings",
            "Optimize website performance metrics",
            "Create a project management workflow"
        ]
    
    async def validate_task_analyzer_optimization(self) -> Dict[str, Any]:
        """Validate TaskAnalyzer optimization performance."""
        print("\nüîç Validating TaskAnalyzer Optimization...")
        
        # Test optimized version
        optimized_analyzer = TaskLevelCachedAnalyzer(cache_size=64)
        optimized_times = []
        
        # Warm up cache
        for task in self.test_tasks[:3]:
            optimized_analyzer.analyze_task(task)
        
        # Measure performance
        for task in self.test_tasks:
            start_time = time.perf_counter()
            result = optimized_analyzer.analyze_task(task)
            end_time = time.perf_counter()
            
            execution_time_ms = (end_time - start_time) * 1000
            optimized_times.append(execution_time_ms)
            
            print(f"  Task analysis: {execution_time_ms:.2f}ms (cached: {getattr(result, 'cache_hit', False)})")
        
        # Test original version for comparison
        original_analyzer = OriginalTaskAnalyzer()
        original_times = []
        
        for task in self.test_tasks:
            start_time = time.perf_counter()
            result = original_analyzer.analyze_task(task)
            end_time = time.perf_counter()
            
            execution_time_ms = (end_time - start_time) * 1000
            original_times.append(execution_time_ms)
        
        avg_optimized = sum(optimized_times) / len(optimized_times)
        avg_original = sum(original_times) / len(original_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100
        
        # Get cache statistics
        cache_info = optimized_analyzer.get_cache_info() if hasattr(optimized_analyzer, 'get_cache_info') else {}
        
        result = {
            "component": "TaskAnalyzer",
            "avg_optimized_time_ms": round(avg_optimized, 2),
            "avg_original_time_ms": round(avg_original, 2),
            "performance_improvement": round(improvement, 1),
            "target_met": avg_optimized < 500,
            "cache_performance": cache_info
        }
        
        print(f"  ‚úÖ Average time: {avg_optimized:.2f}ms (improvement: {improvement:.1f}%)")
        print(f"  üéØ Target <500ms: {'‚úÖ MET' if result['target_met'] else '‚ùå NOT MET'}")
        
        return result
    
    async def validate_tool_discovery_optimization(self) -> Dict[str, Any]:
        """Validate ToolDiscovery parallel processing optimization."""
        print("\nüîß Validating ToolDiscovery Optimization...")
        
        # Mock connection manager for testing
        class MockConnectionManager:
            async def list_connected_servers(self):
                await asyncio.sleep(0.01)  # Simulate network delay
                return ["fetch", "filesystem", "github", "sqlite", "puppeteer"]
            
            async def list_tools(self, server_name):
                await asyncio.sleep(0.005)  # Simulate network delay
                return [f"tool_{i}" for i in range(5)]
            
            async def list_resources(self, server_name):
                await asyncio.sleep(0.005)  # Simulate network delay
                return [f"resource_{i}" for i in range(3)]
            
            async def connect_server(self, server_name):
                await asyncio.sleep(0.01)  # Simulate connection delay
                return True
        
        connection_manager = MockConnectionManager()
        
        # Test optimized version
        optimized_discovery = ParallelToolDiscoveryAgent(
            connection_manager, max_concurrent_operations=5
        )
        
        start_time = time.perf_counter()
        await optimized_discovery.discover_available_servers()
        optimized_time = (time.perf_counter() - start_time) * 1000
        
        # Test original version
        original_discovery = OriginalToolDiscovery(connection_manager)
        
        start_time = time.perf_counter()
        await original_discovery.discover_available_servers()
        original_time = (time.perf_counter() - start_time) * 1000
        
        improvement = ((original_time - optimized_time) / original_time) * 100
        
        # Get performance metrics
        perf_metrics = optimized_discovery.get_performance_summary() if hasattr(optimized_discovery, 'get_performance_summary') else {}
        
        result = {
            "component": "ToolDiscovery",
            "optimized_time_ms": round(optimized_time, 2),
            "original_time_ms": round(original_time, 2),
            "performance_improvement": round(improvement, 1),
            "target_met": optimized_time < 500,
            "parallel_operations": perf_metrics.get("parallel_operations_count", 0),
            "cache_hit_rate": perf_metrics.get("cache_hit_rate", 0)
        }
        
        print(f"  ‚úÖ Discovery time: {optimized_time:.2f}ms (improvement: {improvement:.1f}%)")
        print(f"  üéØ Target <500ms: {'‚úÖ MET' if result['target_met'] else '‚ùå NOT MET'}")
        print(f"  üìä Parallel operations: {result['parallel_operations']}")
        
        return result
    
    async def validate_decision_engine_optimization(self) -> Dict[str, Any]:
        """Validate DecisionEngine caching optimization."""
        print("\nüß† Validating DecisionEngine Optimization...")
        
        # Mock server profiles for testing
        class MockServerProfile:
            def __init__(self, name):
                self.name = name
                self.capabilities = []
        
        available_servers = [MockServerProfile(f"server_{i}") for i in range(3)]
        
        # Test optimized version
        optimized_engine = CachedAutonomousDecisionEngine(enable_cache_stats=True)
        optimized_times = []
        
        # Warm up cache
        for task in self.test_tasks[:3]:
            optimized_engine.analyze_and_recommend(task, available_servers)
        
        # Measure performance
        for task in self.test_tasks:
            start_time = time.perf_counter()
            task_analysis, strategy = optimized_engine.analyze_and_recommend(task, available_servers)
            end_time = time.perf_counter()
            
            execution_time_ms = (end_time - start_time) * 1000
            optimized_times.append(execution_time_ms)
        
        # Test original version
        original_engine = OriginalDecisionEngine()
        original_times = []
        
        for task in self.test_tasks:
            start_time = time.perf_counter()
            task_analysis, strategy = original_engine.analyze_and_recommend(task, available_servers)
            end_time = time.perf_counter()
            
            execution_time_ms = (end_time - start_time) * 1000
            original_times.append(execution_time_ms)
        
        avg_optimized = sum(optimized_times) / len(optimized_times)
        avg_original = sum(original_times) / len(original_times)
        improvement = ((avg_original - avg_optimized) / avg_original) * 100
        
        # Get cache performance
        cache_summary = optimized_engine.get_cache_performance_summary() if hasattr(optimized_engine, 'get_cache_performance_summary') else {}
        
        result = {
            "component": "DecisionEngine", 
            "avg_optimized_time_ms": round(avg_optimized, 2),
            "avg_original_time_ms": round(avg_original, 2),
            "performance_improvement": round(improvement, 1),
            "target_met": avg_optimized < 500,
            "cache_performance": cache_summary
        }
        
        print(f"  ‚úÖ Average time: {avg_optimized:.2f}ms (improvement: {improvement:.1f}%)")
        print(f"  üéØ Target <500ms: {'‚úÖ MET' if result['target_met'] else '‚ùå NOT MET'}")
        if cache_summary.get('hit_rate'):
            print(f"  üíæ Cache hit rate: {cache_summary['hit_rate']:.1f}%")
        
        return result
    
    async def validate_integrated_performance(self) -> Dict[str, Any]:
        """Validate integrated performance of all optimized components."""
        print("\nüöÄ Validating Integrated Performance...")
        
        # Create integrated optimized workflow
        start_time = time.perf_counter()
        
        # Initialize optimized components
        task_analyzer = TaskLevelCachedAnalyzer(cache_size=128)
        
        # Mock connection manager
        class MockConnectionManager:
            async def list_connected_servers(self):
                return ["fetch", "filesystem", "github"]
            async def list_tools(self, server_name):
                return [f"tool_{i}" for i in range(3)]
            async def list_resources(self, server_name):
                return [f"resource_{i}" for i in range(2)]
        
        tool_discovery = ParallelToolDiscoveryAgent(MockConnectionManager())
        decision_engine = CachedAutonomousDecisionEngine()
        
        # Warm up all caches
        mock_servers = [type('MockServer', (), {'name': f'server_{i}', 'capabilities': []})() for i in range(3)]
        
        for task in self.test_tasks[:3]:
            task_analyzer.analyze_task(task)
            decision_engine.analyze_and_recommend(task, mock_servers)
        
        await tool_discovery.discover_available_servers()
        
        initialization_time = (time.perf_counter() - start_time) * 1000
        
        # Test integrated workflow performance
        workflow_times = []
        
        for task in self.test_tasks:
            start_time = time.perf_counter()
            
            # Full integrated workflow
            task_analysis = task_analyzer.analyze_task(task)
            analysis_time = time.perf_counter()
            
            task_decision, strategy = decision_engine.analyze_and_recommend(task, mock_servers)
            decision_time = time.perf_counter()
            
            # Simulate tool discovery (already cached)
            available_servers = tool_discovery.discovered_servers
            discovery_time = time.perf_counter()
            
            total_time = (discovery_time - start_time) * 1000
            workflow_times.append(total_time)
            
            print(f"  Task workflow: {total_time:.2f}ms")
        
        avg_workflow_time = sum(workflow_times) / len(workflow_times)
        
        result = {
            "component": "IntegratedWorkflow",
            "initialization_time_ms": round(initialization_time, 2), 
            "avg_workflow_time_ms": round(avg_workflow_time, 2),
            "target_met": avg_workflow_time < 500,
            "all_targets_met": avg_workflow_time < 500,
            "performance_summary": {
                "fastest_workflow_ms": round(min(workflow_times), 2),
                "slowest_workflow_ms": round(max(workflow_times), 2),
                "consistency": round((max(workflow_times) - min(workflow_times)) / avg_workflow_time * 100, 1)
            }
        }
        
        print(f"  ‚úÖ Average integrated time: {avg_workflow_time:.2f}ms")
        print(f"  üéØ Target <500ms: {'‚úÖ MET' if result['target_met'] else '‚ùå NOT MET'}")
        print(f"  üìà Performance consistency: {result['performance_summary']['consistency']:.1f}% variance")
        
        return result

    def print_validation_summary(self, results: List[Dict[str, Any]]):
        """Print comprehensive validation summary."""
        print("\n" + "="*80)
        print("üéØ PERFORMANCE OPTIMIZATION VALIDATION SUMMARY")
        print("="*80)
        
        total_components = len(results)
        targets_met = sum(1 for r in results if r.get('target_met', False))
        
        for result in results:
            component = result["component"]
            target_status = "‚úÖ MET" if result.get('target_met', False) else "‚ùå NOT MET"
            
            print(f"\nüìä {component}")
            print("-" * 50)
            
            if "avg_optimized_time_ms" in result:
                print(f"Optimized Time:       {result['avg_optimized_time_ms']}ms")
                print(f"Original Time:        {result.get('avg_original_time_ms', 'N/A')}ms")
                print(f"Performance Gain:     {result.get('performance_improvement', 0):.1f}%")
            elif "avg_workflow_time_ms" in result:
                print(f"Workflow Time:        {result['avg_workflow_time_ms']}ms")
                print(f"Initialization:       {result.get('initialization_time_ms', 0)}ms")
            elif "optimized_time_ms" in result:
                print(f"Optimized Time:       {result['optimized_time_ms']}ms")
                print(f"Original Time:        {result.get('original_time_ms', 'N/A')}ms")
                print(f"Performance Gain:     {result.get('performance_improvement', 0):.1f}%")
            
            print(f"<500ms Target:        {target_status}")
            
            # Additional metrics
            if "cache_performance" in result and result["cache_performance"]:
                cache_perf = result["cache_performance"]
                if isinstance(cache_perf, dict) and "hit_rate" in cache_perf:
                    print(f"Cache Hit Rate:       {cache_perf['hit_rate']:.1f}%")
                    
            if "parallel_operations" in result:
                print(f"Parallel Operations:  {result['parallel_operations']}")
        
        # Overall summary
        print(f"\n" + "="*50)
        print("üèÜ OVERALL RESULTS")
        print("="*50)
        print(f"Components Tested:    {total_components}")
        print(f"Targets Met:          {targets_met}/{total_components} ({(targets_met/total_components)*100:.1f}%)")
        
        if targets_met == total_components:
            print("üéâ ALL PERFORMANCE TARGETS MET!")
            print("‚úÖ <500ms response time target achieved across all components")
        else:
            print("‚ö†Ô∏è  Some performance targets not met - additional optimization needed")
        
        # Recommendations
        print(f"\nüìã RECOMMENDATIONS")
        print("-" * 30)
        
        failed_components = [r["component"] for r in results if not r.get('target_met', False)]
        if failed_components:
            print(f"üîß Optimize: {', '.join(failed_components)}")
        else:
            print("‚úÖ All components meeting performance targets")
            print("üí° Consider implementing in production")
            print("üìä Monitor performance in real-world usage")

    async def run_validation(self) -> bool:
        """Run complete validation suite."""
        print("üöÄ Starting Performance Optimization Validation")
        print("=" * 60)
        print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        results = []
        
        try:
            # Validate individual components
            task_analyzer_result = await self.validate_task_analyzer_optimization()
            results.append(task_analyzer_result)
            
            tool_discovery_result = await self.validate_tool_discovery_optimization()
            results.append(tool_discovery_result)
            
            decision_engine_result = await self.validate_decision_engine_optimization()
            results.append(decision_engine_result)
            
            # Validate integrated performance
            integrated_result = await self.validate_integrated_performance()
            results.append(integrated_result)
            
            # Print summary
            self.print_validation_summary(results)
            
            print(f"\n‚úÖ Validation completed successfully!")
            print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            # Return success if all targets met
            return all(r.get('target_met', False) for r in results)
            
        except Exception as e:
            print(f"‚ùå Validation failed: {e}")
            import traceback
            print(f"Traceback: {traceback.format_exc()}")
            return False


async def main():
    """Main validation function."""
    validator = OptimizationValidator()
    success = await validator.run_validation()
    return success


if __name__ == "__main__":
    success = asyncio.run(main())
    print(f"\n{'üéâ SUCCESS' if success else '‚ùå FAILED'}: Performance optimization validation {'completed' if success else 'failed'}")
    sys.exit(0 if success else 1)
